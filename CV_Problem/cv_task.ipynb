{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tso2BzPdxnhM"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import struct\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wn1qiU-nxzf4"
   },
   "outputs": [],
   "source": [
    "with open(\"test_image.pkl\", \"rb\") as f:\n",
    "    testImages = pickle.load(f)\n",
    "\n",
    "with open(\"train_image.pkl\", \"rb\") as f:\n",
    "    trainImages = pickle.load(f)\n",
    "with open(\"train_label.pkl\", \"rb\") as f:\n",
    "    trainLabels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4POLTaZeOLjg"
   },
   "outputs": [],
   "source": [
    "# Replacing class label 6 with 1\n",
    "for i in range(8000):\n",
    "    if(trainLabels[i]==6): \n",
    "        trainLabels[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2Mzpe7C1TYp"
   },
   "outputs": [],
   "source": [
    "trainImages = np.asarray(trainImages)\n",
    "trainImages = trainImages.reshape(-1,1,28,28)\n",
    "testImages = np.asarray(testImages)\n",
    "testImages = testImages.reshape(-1,1,28,28)\n",
    "\n",
    "trainImages = (torch.from_numpy(trainImages)).float()\n",
    "trainLabels = torch.from_numpy(np.asarray(trainLabels))\n",
    "testImages = (torch.from_numpy(np.asarray(testImages))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bV4pWM7t0s5b"
   },
   "outputs": [],
   "source": [
    "# plt.imshow(trainImages[0].reshape((28,28)))\n",
    "# print(trainLabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbb5AaT2Jsp-"
   },
   "outputs": [],
   "source": [
    "BatchSize = 100\n",
    "\n",
    "trainset = torch.utils.data.TensorDataset(trainImages, trainLabels)\n",
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size=BatchSize, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "G7RmWn7xLlyR",
    "outputId": "776593e1-2e27-4e03-f89f-7168ee8ea990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of samples in train set: 8000\n"
     ]
    }
   ],
   "source": [
    "print('No. of samples in train set: '+str(len(trainLoader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining network architecture\n",
    "#### CNN LeNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJbgFAB4NHVH"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)        \n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "n5PE7IGkTyOm",
    "outputId": "c75e6516-a9c1-493a-ab1b-0862caeb4280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ApmS5dA8T1R7",
    "outputId": "115cf69d-6d71-432a-ba2d-f5848d2ece12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU unavailable !\n"
     ]
    }
   ],
   "source": [
    "# Check availability of GPU\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is available !')   \n",
    "    net = net.cuda()\n",
    "else:\n",
    "    print('GPU unavailable !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtHSopXiWCJ5"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() # Negative Log-likelihood\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-5) # Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1521
    },
    "colab_type": "code",
    "id": "JLtzO3dgWICX",
    "outputId": "2dfc6159-7ef4-44d0-80e8-51d01f04c26c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 /100  ;  Training Loss: 0.012481 ; Time consumed: 0m 1s \n",
      "Iteration: 2 /100  ;  Training Loss: 0.010075 ; Time consumed: 0m 1s \n",
      "Iteration: 3 /100  ;  Training Loss: 0.009009 ; Time consumed: 0m 1s \n",
      "Iteration: 4 /100  ;  Training Loss: 0.008415 ; Time consumed: 0m 1s \n",
      "Iteration: 5 /100  ;  Training Loss: 0.008045 ; Time consumed: 0m 1s \n",
      "Iteration: 6 /100  ;  Training Loss: 0.007750 ; Time consumed: 0m 1s \n",
      "Iteration: 7 /100  ;  Training Loss: 0.007523 ; Time consumed: 0m 1s \n",
      "Iteration: 8 /100  ;  Training Loss: 0.007336 ; Time consumed: 0m 1s \n",
      "Iteration: 9 /100  ;  Training Loss: 0.007176 ; Time consumed: 0m 1s \n",
      "Iteration: 10 /100  ;  Training Loss: 0.007023 ; Time consumed: 0m 1s \n",
      "Iteration: 11 /100  ;  Training Loss: 0.006900 ; Time consumed: 0m 1s \n",
      "Iteration: 12 /100  ;  Training Loss: 0.006778 ; Time consumed: 0m 1s \n",
      "Iteration: 13 /100  ;  Training Loss: 0.006676 ; Time consumed: 0m 1s \n",
      "Iteration: 14 /100  ;  Training Loss: 0.006584 ; Time consumed: 0m 1s \n",
      "Iteration: 15 /100  ;  Training Loss: 0.006487 ; Time consumed: 0m 1s \n",
      "Iteration: 16 /100  ;  Training Loss: 0.006407 ; Time consumed: 0m 1s \n",
      "Iteration: 17 /100  ;  Training Loss: 0.006320 ; Time consumed: 0m 1s \n",
      "Iteration: 18 /100  ;  Training Loss: 0.006262 ; Time consumed: 0m 1s \n",
      "Iteration: 19 /100  ;  Training Loss: 0.006189 ; Time consumed: 0m 1s \n",
      "Iteration: 20 /100  ;  Training Loss: 0.006105 ; Time consumed: 0m 1s \n",
      "Iteration: 21 /100  ;  Training Loss: 0.006033 ; Time consumed: 0m 1s \n",
      "Iteration: 22 /100  ;  Training Loss: 0.005974 ; Time consumed: 0m 1s \n",
      "Iteration: 23 /100  ;  Training Loss: 0.005906 ; Time consumed: 0m 1s \n",
      "Iteration: 24 /100  ;  Training Loss: 0.005841 ; Time consumed: 0m 1s \n",
      "Iteration: 25 /100  ;  Training Loss: 0.005780 ; Time consumed: 0m 1s \n",
      "Iteration: 26 /100  ;  Training Loss: 0.005709 ; Time consumed: 0m 1s \n",
      "Iteration: 27 /100  ;  Training Loss: 0.005648 ; Time consumed: 0m 1s \n",
      "Iteration: 28 /100  ;  Training Loss: 0.005604 ; Time consumed: 0m 1s \n",
      "Iteration: 29 /100  ;  Training Loss: 0.005550 ; Time consumed: 0m 1s \n",
      "Iteration: 30 /100  ;  Training Loss: 0.005504 ; Time consumed: 0m 1s \n",
      "Iteration: 31 /100  ;  Training Loss: 0.005455 ; Time consumed: 0m 1s \n",
      "Iteration: 32 /100  ;  Training Loss: 0.005401 ; Time consumed: 0m 1s \n",
      "Iteration: 33 /100  ;  Training Loss: 0.005359 ; Time consumed: 0m 1s \n",
      "Iteration: 34 /100  ;  Training Loss: 0.005330 ; Time consumed: 0m 1s \n",
      "Iteration: 35 /100  ;  Training Loss: 0.005286 ; Time consumed: 0m 1s \n",
      "Iteration: 36 /100  ;  Training Loss: 0.005243 ; Time consumed: 0m 1s \n",
      "Iteration: 37 /100  ;  Training Loss: 0.005198 ; Time consumed: 0m 1s \n",
      "Iteration: 38 /100  ;  Training Loss: 0.005181 ; Time consumed: 0m 1s \n",
      "Iteration: 39 /100  ;  Training Loss: 0.005137 ; Time consumed: 0m 1s \n",
      "Iteration: 40 /100  ;  Training Loss: 0.005108 ; Time consumed: 0m 1s \n",
      "Iteration: 41 /100  ;  Training Loss: 0.005084 ; Time consumed: 0m 1s \n",
      "Iteration: 42 /100  ;  Training Loss: 0.005050 ; Time consumed: 0m 1s \n",
      "Iteration: 43 /100  ;  Training Loss: 0.005021 ; Time consumed: 0m 1s \n",
      "Iteration: 44 /100  ;  Training Loss: 0.004990 ; Time consumed: 0m 1s \n",
      "Iteration: 45 /100  ;  Training Loss: 0.004969 ; Time consumed: 0m 1s \n",
      "Iteration: 46 /100  ;  Training Loss: 0.004934 ; Time consumed: 0m 1s \n",
      "Iteration: 47 /100  ;  Training Loss: 0.004911 ; Time consumed: 0m 1s \n",
      "Iteration: 48 /100  ;  Training Loss: 0.004887 ; Time consumed: 0m 1s \n",
      "Iteration: 49 /100  ;  Training Loss: 0.004862 ; Time consumed: 0m 1s \n",
      "Iteration: 50 /100  ;  Training Loss: 0.004839 ; Time consumed: 0m 1s \n",
      "Iteration: 51 /100  ;  Training Loss: 0.004822 ; Time consumed: 0m 1s \n",
      "Iteration: 52 /100  ;  Training Loss: 0.004805 ; Time consumed: 0m 1s \n",
      "Iteration: 53 /100  ;  Training Loss: 0.004772 ; Time consumed: 0m 1s \n",
      "Iteration: 54 /100  ;  Training Loss: 0.004743 ; Time consumed: 0m 1s \n",
      "Iteration: 55 /100  ;  Training Loss: 0.004729 ; Time consumed: 0m 1s \n",
      "Iteration: 56 /100  ;  Training Loss: 0.004700 ; Time consumed: 0m 1s \n",
      "Iteration: 57 /100  ;  Training Loss: 0.004697 ; Time consumed: 0m 1s \n",
      "Iteration: 58 /100  ;  Training Loss: 0.004667 ; Time consumed: 0m 1s \n",
      "Iteration: 59 /100  ;  Training Loss: 0.004641 ; Time consumed: 0m 1s \n",
      "Iteration: 60 /100  ;  Training Loss: 0.004632 ; Time consumed: 0m 1s \n",
      "Iteration: 61 /100  ;  Training Loss: 0.004601 ; Time consumed: 0m 1s \n",
      "Iteration: 62 /100  ;  Training Loss: 0.004604 ; Time consumed: 0m 1s \n",
      "Iteration: 63 /100  ;  Training Loss: 0.004565 ; Time consumed: 0m 1s \n",
      "Iteration: 64 /100  ;  Training Loss: 0.004550 ; Time consumed: 0m 1s \n",
      "Iteration: 65 /100  ;  Training Loss: 0.004529 ; Time consumed: 0m 1s \n",
      "Iteration: 66 /100  ;  Training Loss: 0.004505 ; Time consumed: 0m 1s \n",
      "Iteration: 67 /100  ;  Training Loss: 0.004488 ; Time consumed: 0m 1s \n",
      "Iteration: 68 /100  ;  Training Loss: 0.004475 ; Time consumed: 0m 1s \n",
      "Iteration: 69 /100  ;  Training Loss: 0.004444 ; Time consumed: 0m 1s \n",
      "Iteration: 70 /100  ;  Training Loss: 0.004440 ; Time consumed: 0m 1s \n",
      "Iteration: 71 /100  ;  Training Loss: 0.004425 ; Time consumed: 0m 1s \n",
      "Iteration: 72 /100  ;  Training Loss: 0.004400 ; Time consumed: 0m 1s \n",
      "Iteration: 73 /100  ;  Training Loss: 0.004408 ; Time consumed: 0m 1s \n",
      "Iteration: 74 /100  ;  Training Loss: 0.004369 ; Time consumed: 0m 1s \n",
      "Iteration: 75 /100  ;  Training Loss: 0.004358 ; Time consumed: 0m 1s \n",
      "Iteration: 76 /100  ;  Training Loss: 0.004339 ; Time consumed: 0m 1s \n",
      "Iteration: 77 /100  ;  Training Loss: 0.004333 ; Time consumed: 0m 1s \n",
      "Iteration: 78 /100  ;  Training Loss: 0.004314 ; Time consumed: 0m 1s \n",
      "Iteration: 79 /100  ;  Training Loss: 0.004290 ; Time consumed: 0m 1s \n",
      "Iteration: 80 /100  ;  Training Loss: 0.004270 ; Time consumed: 0m 1s \n",
      "Iteration: 81 /100  ;  Training Loss: 0.004252 ; Time consumed: 0m 1s \n",
      "Iteration: 82 /100  ;  Training Loss: 0.004248 ; Time consumed: 0m 1s \n",
      "Iteration: 83 /100  ;  Training Loss: 0.004237 ; Time consumed: 0m 1s \n",
      "Iteration: 84 /100  ;  Training Loss: 0.004214 ; Time consumed: 0m 1s \n",
      "Iteration: 85 /100  ;  Training Loss: 0.004203 ; Time consumed: 0m 1s \n",
      "Iteration: 86 /100  ;  Training Loss: 0.004187 ; Time consumed: 0m 1s \n",
      "Iteration: 87 /100  ;  Training Loss: 0.004178 ; Time consumed: 0m 1s \n",
      "Iteration: 88 /100  ;  Training Loss: 0.004153 ; Time consumed: 0m 1s \n",
      "Iteration: 89 /100  ;  Training Loss: 0.004143 ; Time consumed: 0m 1s \n",
      "Iteration: 90 /100  ;  Training Loss: 0.004129 ; Time consumed: 0m 1s \n",
      "Iteration: 91 /100  ;  Training Loss: 0.004121 ; Time consumed: 0m 1s \n",
      "Iteration: 92 /100  ;  Training Loss: 0.004098 ; Time consumed: 0m 1s \n",
      "Iteration: 93 /100  ;  Training Loss: 0.004121 ; Time consumed: 0m 1s \n",
      "Iteration: 94 /100  ;  Training Loss: 0.004072 ; Time consumed: 0m 1s \n",
      "Iteration: 95 /100  ;  Training Loss: 0.004084 ; Time consumed: 0m 1s \n",
      "Iteration: 96 /100  ;  Training Loss: 0.004053 ; Time consumed: 0m 1s \n",
      "Iteration: 97 /100  ;  Training Loss: 0.004044 ; Time consumed: 0m 1s \n",
      "Iteration: 98 /100  ;  Training Loss: 0.004029 ; Time consumed: 0m 1s \n",
      "Iteration: 99 /100  ;  Training Loss: 0.004033 ; Time consumed: 0m 1s \n",
      "Iteration: 100 /100  ;  Training Loss: 0.003994 ; Time consumed: 0m 1s \n",
      "Training completed in 1m 54s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = 100\n",
    "trainLoss = []\n",
    "start = time.time()\n",
    "for epoch in range(iterations):\n",
    "    epochStart = time.time()\n",
    "    runningLoss = 0    \n",
    "    net.train(True) # For training\n",
    "    for data in trainLoader:\n",
    "        inputs,labels = data\n",
    "        # Wrap them in Variable\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), \\\n",
    "                Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)  \n",
    "       \n",
    "        # Initialize gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Feed-forward input data through the network        \n",
    "        outputs = net(inputs)        \n",
    "        # Compute loss/error\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backpropagate loss and compute gradients\n",
    "        loss.backward()\n",
    "        # Update the network parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate loss per batch\n",
    "        runningLoss += loss.data \n",
    "    avgTrainLoss = runningLoss/8000.0\n",
    "    trainLoss.append(avgTrainLoss)\n",
    "        \n",
    "    # Plotting training loss vs Epochs\n",
    "    fig1 = plt.figure(1)        \n",
    "    plt.plot(range(epoch+1),trainLoss,'r-',label='train')        \n",
    "    if epoch==0:\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Training loss')       \n",
    "    epochEnd = time.time()-epochStart\n",
    "    print('Iteration: {:.0f} /{:.0f}  ;  Training Loss: {:.6f} ; Time consumed: {:.0f}m {:.0f}s '\\\n",
    "          .format(epoch + 1,iterations,avgTrainLoss,epochEnd//60,epochEnd%60))\n",
    "end = time.time()-start\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(end//60,end%60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting test labels and saving into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "rDr6jrcsNZms",
    "outputId": "420837bf-1134-4a43-d9d7-ab10bf51077e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harshit/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "if use_gpu:\n",
    "    inputs = Variable(testImages.cuda())  \n",
    "else:\n",
    "    inputs = Variable(testImages)\n",
    "\n",
    "outputs = net(inputs)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "finalPred = predicted.cpu().numpy()\n",
    "\n",
    "# Replacing class label 1 with 6\n",
    "for i in range(2000):\n",
    "    if finalPred[i] == 1:\n",
    "        finalPred[i] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmem06IS2eHe"
   },
   "outputs": [],
   "source": [
    "# Saving predictions into a file\n",
    "x = np.arange(0, 2000, 1)\n",
    "x.astype(int)\n",
    "final = np.asarray((x,finalPred),dtype = int)\n",
    "np.savetxt(\"Testlabel.csv\",final.T,fmt='%d' ,delimiter=',', header='image,pred',comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TaskCV.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
